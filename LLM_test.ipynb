{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf8eI3BPvmfE",
        "outputId": "31f7669a-576f-466c-bce2-e60662f9f249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive (only run if not mounted)\n",
        "# ensures the Colab session can read/write the model and test files in Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "============================================================\n",
        " test.py\n",
        " -----------------------------------------------------------\n",
        " PURPOSE:\n",
        "   - Load trained DistilBERT model from local directory\n",
        "   - Run inference on test dataset (IMDB or custom examples)\n",
        "   - Save outputs (reports, metrics, misclassifications) to Drive\n",
        "   - Ensure reproducibility and auditability for downstream governance\n",
        "\n",
        " AI SECURITY & GOVERNANCE:\n",
        "   - Dataset Integrity: SHA256 hash of test CSV logged for tamper detection\n",
        "   - Provenance: Training metadata.json included in test reports\n",
        "   - Audit Logs: Human-readable + JSON reports generated\n",
        "   - Environment Logging: Python, Torch, CUDA info saved\n",
        "   - Misclassification Analysis: Saved for error inspection\n",
        "   - Reproducibility: All outputs stored in model_dir/test_results/\n",
        "\n",
        " DEVELOPMENT NOTE:\n",
        "   - For Colab, we temporarily load model + data from Drive.\n",
        "   - Final deployment will use Docker with embedded model,\n",
        "     ensuring offline reproducibility without Drive/HF dependencies.\n",
        "============================================================\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "xtpETpVexPfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set paths and ensure the model folder exists\n",
        "# Centralize locations and keep outputs inside the model folder for provenance\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "MODEL_DIR = Path(\"/content/drive/MyDrive/quick_distilbert_model\")  # adjust if different\n",
        "TEST_CSV  = Path(\"/content/drive/MyDrive/quick_distilbert_model/imdb_test_clean.csv\")     # path to test dataset on Drive\n",
        "TEST_RESULTS_DIR = MODEL_DIR / \"test_results\"                      # where test outputs will be saved\n",
        "\n",
        "# Create results folder if it doesn't exist\n",
        "TEST_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Sanity checks\n",
        "print(\"Model folder exists:\", MODEL_DIR.exists())\n",
        "print(\"Test CSV exists:\", TEST_CSV.exists())\n",
        "print(\"Test results folder:\", TEST_RESULTS_DIR)\n",
        "print(\"Listing model folder sample files:\", os.listdir(MODEL_DIR)[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOwGsgPvxRPX",
        "outputId": "ad7a149f-b03c-414e-b540-83c345a33ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model folder exists: True\n",
            "Test CSV exists: True\n",
            "Test results folder: /content/drive/MyDrive/quick_distilbert_model/test_results\n",
            "Listing model folder sample files: ['imdb_test_clean.csv', 'config.json', 'model.safetensors', 'tokenizer_config.json', 'special_tokens_map.json', 'vocab.txt', 'tokenizer.json', 'training_args.bin', 'metadata.json', 'eval_outputs', 'test_results']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Governance utilities\n",
        "# These help with reproducibility and audit trails (dataset hashes, metadata, environment)\n",
        "\n",
        "import json, hashlib, platform, datetime\n",
        "\n",
        "def file_hash(path: Path) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def load_metadata(model_dir: Path):\n",
        "    md = model_dir / \"metadata.json\"\n",
        "    if md.exists():\n",
        "        try:\n",
        "            return json.loads(md.read_text())\n",
        "        except Exception as e:\n",
        "            print(f\"[GOV WARN] metadata.json exists but could not be parsed: {e}\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def env_info():\n",
        "    return {\n",
        "        \"python\": platform.python_version(),\n",
        "        \"torch_cuda_available\": torch.cuda.is_available(),\n",
        "        \"torch_cuda_device_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
        "        \"platform\": platform.platform(),\n",
        "        \"timestamp_utc\": datetime.datetime.utcnow().isoformat() + \"Z\"\n",
        "    }\n",
        "\n",
        "# quick gov output\n",
        "metadata = load_metadata(MODEL_DIR)\n",
        "print(\"Loaded metadata?:\", True if metadata else False)\n"
      ],
      "metadata": {
        "id": "J-K28XnOxS24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5288ef-d12a-4fdf-f5df-f14bf5c771f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded metadata?: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer (local_files_only=True)\n",
        "# we are using the Drive copy for development; avoid HF hub calls for now.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"[INFO] Using device:\", device)\n",
        "\n",
        "# Load (this will read from MODEL_DIR contents)\n",
        "tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR), local_files_only=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(str(MODEL_DIR), local_files_only=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"[INFO] Loaded tokenizer and model from Drive.\")\n"
      ],
      "metadata": {
        "id": "hFMOUxG5xTEn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c064e8-3d2d-4965-8a11-2089acfa3735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using device: cuda\n",
            "[INFO] Loaded tokenizer and model from Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Prepare test inputs\n",
        "# support running a full CSV or a small set of custom examples for smoke/adversarial checks\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "USE_CSV = True   # set False to run custom_examples below instead of CSV\n",
        "custom_examples = [\n",
        "    \"I absolutely loved this movie, it was brilliant and moving.\",\n",
        "    \"Worst movie ever. Boring plot and bad acting.\",\n",
        "    \"\",  # empty input test\n",
        "    \"<script>alert('x')</script>\",\n",
        "    \"üòäüëç\"\n",
        "]\n",
        "\n",
        "if USE_CSV:\n",
        "    if not TEST_CSV.exists():\n",
        "        raise FileNotFoundError(f\"Test CSV not found at {TEST_CSV}\")\n",
        "    df_test = pd.read_csv(TEST_CSV)\n",
        "    # Ensure expected columns\n",
        "    if \"review\" not in df_test.columns or \"sentiment\" not in df_test.columns:\n",
        "        raise ValueError(\"Test CSV must contain 'review' and 'sentiment' columns.\")\n",
        "    texts = df_test[\"review\"].astype(str).tolist()\n",
        "    true_labels = df_test[\"sentiment\"].astype(int).tolist()\n",
        "else:\n",
        "    texts = custom_examples\n",
        "    true_labels = [None] * len(texts)  # unknown labels for custom tests\n",
        "\n",
        "print(f\"[INFO] Number of test examples: {len(texts)}\")\n"
      ],
      "metadata": {
        "id": "4I26FJzBxTRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad92936-f73d-4c4e-9f95-7f558e403c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Number of test examples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batched inference loop (runs on device)\n",
        "# Efficient inference with batching; collects probs, preds, and logs\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SimpleTextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        t = str(self.texts[idx])\n",
        "        enc = self.tokenizer(t, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item[\"text\"] = t\n",
        "        return item\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "dataset = SimpleTextDataset(texts, tokenizer, max_length=256)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "all_preds = []\n",
        "all_probs = []\n",
        "all_texts = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(dataloader, desc=\"Running Tests\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = softmax(logits).cpu().numpy()\n",
        "        preds = np.argmax(probs, axis=1).tolist()\n",
        "        all_preds.extend(preds)\n",
        "        all_probs.extend(probs.tolist())\n",
        "        all_texts.extend(batch[\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-I-XWSJF6xT",
        "outputId": "15435c62-8e54-4952-e039-e8eced5e6e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running Tests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [01:17<00:00,  4.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics and report creation\n",
        "# compute metrics if labels exist, otherwise produce inference-only log\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "has_labels = all(l is not None for l in true_labels)\n",
        "\n",
        "if has_labels:\n",
        "    # Only keep the same length (CSV case)\n",
        "    assert len(true_labels) == len(all_preds)\n",
        "    acc = accuracy_score(true_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(true_labels, all_preds, average=\"binary\", zero_division=0)\n",
        "    class_report = classification_report(true_labels, all_preds, digits=4, zero_division=0)\n",
        "    cm = confusion_matrix(true_labels, all_preds)\n",
        "else:\n",
        "    acc = prec = rec = f1 = None\n",
        "    class_report = None\n",
        "    cm = None\n",
        "\n",
        "# Build test summary for human + audit\n",
        "test_timestamp = datetime.datetime.utcnow().isoformat() + \"Z\"\n",
        "test_summary_txt = f\"Test run timestamp: {test_timestamp}\\nModel dir: {MODEL_DIR}\\nDevice: {device}\\n\"\n",
        "if has_labels:\n",
        "    test_summary_txt += f\"Accuracy: {acc:.4f}\\nPrecision: {prec:.4f}\\nRecall: {rec:.4f}\\nF1: {f1:.4f}\\n\\n\"\n",
        "    test_summary_txt += \"Classification report:\\n\" + class_report + \"\\n\"\n",
        "    test_summary_txt += \"Confusion matrix:\\n\" + (str(cm) if cm is not None else \"None\") + \"\\n\"\n",
        "else:\n",
        "    test_summary_txt += \"No true labels provided (inference-only run).\\n\"\n",
        "\n",
        "# Structured JSON summary (machine-readable)\n",
        "test_summary_json = {\n",
        "    \"timestamp_utc\": test_timestamp,\n",
        "    \"model_dir\": str(MODEL_DIR),\n",
        "    \"device\": device,\n",
        "    \"num_examples\": len(all_texts),\n",
        "    \"has_labels\": has_labels,\n",
        "    \"metrics\": {\n",
        "        \"accuracy\": float(acc) if acc is not None else None,\n",
        "        \"precision\": float(prec) if prec is not None else None,\n",
        "        \"recall\": float(rec) if rec is not None else None,\n",
        "        \"f1\": float(f1) if f1 is not None else None\n",
        "    },\n",
        "    \"env\": env_info(),\n",
        "    \"metadata_present\": bool(metadata),\n",
        "    \"metadata\": metadata if metadata else None,\n",
        "    \"test_csv_hash\": file_hash(TEST_CSV) if TEST_CSV.exists() else None\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l42grzFBGPt1",
        "outputId": "94df7e26-581e-4dce-c8a4-40c08eb2914c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2941629290.py:22: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  test_timestamp = datetime.datetime.utcnow().isoformat() + \"Z\"\n",
            "/tmp/ipython-input-3602460099.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"timestamp_utc\": datetime.datetime.utcnow().isoformat() + \"Z\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save outputs to Drive inside the model folder\n",
        "# centralize model + evaluation + test artifacts for reproducibility\n",
        "\n",
        "import csv, json\n",
        "out_dir = TEST_RESULTS_DIR\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) human-readable txt\n",
        "txt_path = out_dir / \"test_report.txt\"\n",
        "with open(txt_path, \"w\") as f:\n",
        "    f.write(test_summary_txt)\n",
        "print(f\"[INFO] Saved human-readable test report -> {txt_path}\")\n",
        "\n",
        "# 2) structured JSON\n",
        "json_path = out_dir / f\"test_report_{test_timestamp.replace(':','-')}.json\"\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(test_summary_json, f, indent=2)\n",
        "print(f\"[INFO] Saved structured test report -> {json_path}\")\n",
        "\n",
        "# 3) predictions CSV (all inputs, preds, prob scores, optional true labels)\n",
        "preds_df = pd.DataFrame({\n",
        "    \"text\": all_texts,\n",
        "    \"pred_label\": all_preds,\n",
        "    \"pred_scores\": [\", \".join([f\"{s:.4f}\" for s in p]) for p in all_probs]\n",
        "})\n",
        "if has_labels:\n",
        "    preds_df[\"true_label\"] = true_labels\n",
        "\n",
        "preds_csv_path = out_dir / \"test_predictions.csv\"\n",
        "preds_df.to_csv(preds_csv_path, index=False)\n",
        "print(f\"[INFO] Saved predictions CSV -> {preds_csv_path}\")\n",
        "\n",
        "# 4) misclassified examples (if labels available)\n",
        "if has_labels:\n",
        "    mis_df = preds_df[preds_df[\"true_label\"] != preds_df[\"pred_label\"]].copy()\n",
        "    mis_csv_path = out_dir / \"test_misclassified.csv\"\n",
        "    mis_df.to_csv(mis_csv_path, index=False)\n",
        "    print(f\"[INFO] Saved misclassified examples -> {mis_csv_path}\")\n",
        "else:\n",
        "    print(\"[INFO] No true labels available ‚Äî skipping misclassified examples file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuwl1EJyGYPX",
        "outputId": "110c6067-f122-4aab-dec6-4c7ab83f17e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saved human-readable test report -> /content/drive/MyDrive/quick_distilbert_model/test_results/test_report.txt\n",
            "[INFO] Saved structured test report -> /content/drive/MyDrive/quick_distilbert_model/test_results/test_report_2025-09-13T12-42-44.187470Z.json\n",
            "[INFO] Saved predictions CSV -> /content/drive/MyDrive/quick_distilbert_model/test_results/test_predictions.csv\n",
            "[INFO] Saved misclassified examples -> /content/drive/MyDrive/quick_distilbert_model/test_results/test_misclassified.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save a compact run log with sample entries (JSON)\n",
        "# a single artifact that includes sample predictions for quick review\n",
        "\n",
        "run_log = {\n",
        "    \"timestamp_utc\": test_timestamp,\n",
        "    \"num_examples\": len(all_texts),\n",
        "    \"sample_predictions\": [\n",
        "        {\"text\": all_texts[i], \"pred_label\": int(all_preds[i]), \"pred_scores\": all_probs[i],\n",
        "         \"true_label\": (int(true_labels[i]) if true_labels[i] is not None else None)}\n",
        "        for i in range(min(20, len(all_texts)))\n",
        "    ],\n",
        "    \"report_json\": test_summary_json\n",
        "}\n",
        "run_log_path = out_dir / f\"test_run_log_{test_timestamp.replace(':','-')}.json\"\n",
        "with open(run_log_path, \"w\") as f:\n",
        "    json.dump(run_log, f, indent=2)\n",
        "print(f\"[INFO] Saved run log -> {run_log_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSYUaHQEGimm",
        "outputId": "47d15147-cdbe-4106-90c1-b71a22d189f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saved run log -> /content/drive/MyDrive/quick_distilbert_model/test_results/test_run_log_2025-09-13T12-42-44.187470Z.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print summary and show saved files\n",
        "\n",
        "print(\"=== TEST RUN COMPLETE ===\")\n",
        "print(\"Saved files in\", TEST_RESULTS_DIR)\n",
        "print(list(TEST_RESULTS_DIR.iterdir()))\n",
        "# show first lines of human-readable report\n",
        "print(\"\\n--- Report preview ---\\n\")\n",
        "print(open(txt_path).read()[:2000])\n"
      ],
      "metadata": {
        "id": "0pDJhfmdGmEC",
        "outputId": "4040a6cb-3837-489e-9c1d-1397003f1fcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TEST RUN COMPLETE ===\n",
            "Saved files in /content/drive/MyDrive/quick_distilbert_model/test_results\n",
            "[PosixPath('/content/drive/MyDrive/quick_distilbert_model/test_results/test_report.txt'), PosixPath('/content/drive/MyDrive/quick_distilbert_model/test_results/test_report_2025-09-13T12-42-44.187470Z.json'), PosixPath('/content/drive/MyDrive/quick_distilbert_model/test_results/test_predictions.csv'), PosixPath('/content/drive/MyDrive/quick_distilbert_model/test_results/test_misclassified.csv'), PosixPath('/content/drive/MyDrive/quick_distilbert_model/test_results/test_run_log_2025-09-13T12-42-44.187470Z.json')]\n",
            "\n",
            "--- Report preview ---\n",
            "\n",
            "Test run timestamp: 2025-09-13T12:42:44.187470Z\n",
            "Model dir: /content/drive/MyDrive/quick_distilbert_model\n",
            "Device: cuda\n",
            "Accuracy: 0.8249\n",
            "Precision: 0.8053\n",
            "Recall: 0.8570\n",
            "F1: 0.8303\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8472    0.7928    0.8191      5000\n",
            "           1     0.8053    0.8570    0.8303      5000\n",
            "\n",
            "    accuracy                         0.8249     10000\n",
            "   macro avg     0.8262    0.8249    0.8247     10000\n",
            "weighted avg     0.8262    0.8249    0.8247     10000\n",
            "\n",
            "Confusion matrix:\n",
            "[[3964 1036]\n",
            " [ 715 4285]]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}