{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HmcIiy7c7c8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  Evaluate_model.py\n",
        "  -------------------------------------------------------\n",
        "  PURPOSE:\n",
        "\n",
        "    - Load trained model from disk\n",
        "\n",
        "    - Evaluate on test set\n",
        "\n",
        "    - Save metrics (accuracy, precision, recall, F1)\n",
        "\n",
        "    - Save confusion matrix and misclassified examples\n",
        "\n",
        "\n",
        "  SECURITY & GOVERNANCE NOTES:\n",
        "\n",
        "    - Keep test dataset hashes to confirm integrity\n",
        "\n",
        "    - Save metrics in JSON for auditability\n",
        "\n",
        "    - Misclassified samples saved for error analysis\n"
      ],
      "metadata": {
        "id": "dcJekYf8YrkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Temporary (Colab / Development phase):**\n",
        "For now, we are loading the trained model directly from Google Drive into Colab.\n",
        "\n",
        "This allows quick access and faster iteration.\n",
        "\n",
        "Evaluation metrics (eval_report.txt, eval_report.json, etc.) are generated from the Drive copy.\n",
        "\n",
        "Governance checks (metadata.json, dataset hashes) are still enforced to ensure integrity.\n",
        "\n",
        "Final (Docker / Deployment phase):\n",
        "Once training, evaluation, and testing are finalized, we will embed the model inside a Docker image.\n",
        "\n",
        "#Recruiters, teammates, or reviewers can run:\n",
        "\n",
        "docker build -t imdb-sentiment\n",
        "docker run --rm imdb-sentiment\n",
        "\n",
        "\n",
        "#The Docker image will contain:\n",
        "\n",
        "- Model weights (quick_distilbert_model/)\n",
        "\n",
        "- Metadata (metadata.json for auditability)\n",
        "\n",
        "- Scripts (train_model.py, evaluate_model.py, test_model.py)\n",
        "\n",
        "This ensures reproducibility, offline use, and no dependency on Google Drive.\n",
        "\n",
        "✅ Summary:\n",
        "Drive is used now for development convenience, but Docker will be the final, self-contained delivery format for reproducibility and professional deployment."
      ],
      "metadata": {
        "id": "6AQPpnHF9ygM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jXBTIM7AGqz",
        "outputId": "4793d8b3-491a-43bc-bdc9-a987617cca51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/quick_distilbert_model\"   # trained model folder\n",
        "TEST_CSV  = \"/content/drive/MyDrive/quick_distilbert_model/imdb_test_clean.csv\"      # test dataset\n",
        "SAVE_DIR  = \"/content/drive/MyDrive/quick_distilbert_model/eval_outputs\"             # save results here\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "58FQeXOhiSvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, hashlib, datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Utility: compute file hash (AI governance)\n",
        "def file_hash(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "# Minimal Dataset wrapper\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item[\"labels\"] = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
        "        item[\"text\"] = text\n",
        "        return item\n",
        "\n",
        "# Evaluation function\n",
        "def run_evaluation(model_dir, test_csv, save_dir, batch_size=32, max_length=256):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"[INFO] Using device: {device}\")\n",
        "\n",
        "    # Load model + tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir, local_files_only=True)\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # Load test data\n",
        "    df = pd.read_csv(test_csv)\n",
        "    texts = df[\"review\"].astype(str).tolist()\n",
        "    labels = df[\"sentiment\"].astype(int).tolist()\n",
        "\n",
        "    dataset = TextDataset(texts, labels, tokenizer, max_length=max_length)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Run predictions\n",
        "    preds, true_labels, probs, texts_record = [], [], [], []\n",
        "    softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            ids = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            batch_probs = softmax(out.logits).cpu().numpy()\n",
        "            batch_preds = np.argmax(batch_probs, axis=1).tolist()\n",
        "\n",
        "            preds.extend(batch_preds)\n",
        "            probs.extend(batch_probs.tolist())\n",
        "            true_labels.extend(batch[\"labels\"].numpy().tolist())\n",
        "            texts_record.extend(batch[\"text\"])\n",
        "\n",
        "    # Metrics\n",
        "    acc = accuracy_score(true_labels, preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(true_labels, preds, average=\"binary\")\n",
        "    report = classification_report(true_labels, preds, digits=4)\n",
        "    cm = confusion_matrix(true_labels, preds)\n",
        "\n",
        "    # Save human-readable report\n",
        "    txt_path = os.path.join(save_dir, \"eval_report.txt\")\n",
        "    with open(txt_path, \"w\") as f:\n",
        "        f.write(f\"Accuracy: {acc:.4f}\\nPrecision: {prec:.4f}\\nRecall: {rec:.4f}\\nF1: {f1:.4f}\\n\\n\")\n",
        "        f.write(\"Classification Report:\\n\" + report + \"\\n\")\n",
        "        f.write(\"Confusion Matrix:\\n\" + str(cm))\n",
        "\n",
        "    # Save structured JSON (for governance/audit)\n",
        "    json_path = os.path.join(save_dir, \"eval_report.json\")\n",
        "    metrics = {\n",
        "        \"timestamp\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"accuracy\": float(acc),\n",
        "        \"precision\": float(prec),\n",
        "        \"recall\": float(rec),\n",
        "        \"f1\": float(f1),\n",
        "        \"test_csv_hash\": file_hash(test_csv)\n",
        "    }\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    # Save predictions + misclassified\n",
        "    df_out = pd.DataFrame({\"text\": texts_record, \"true\": true_labels, \"pred\": preds})\n",
        "    df_out.to_csv(os.path.join(save_dir, \"predictions_log.csv\"), index=False)\n",
        "    mis = df_out[df_out[\"true\"] != df_out[\"pred\"]]\n",
        "    mis.to_csv(os.path.join(save_dir, \"misclassified_examples.csv\"), index=False)\n",
        "\n",
        "    print(f\"[INFO] ✅ Evaluation complete. Reports saved to: {save_dir}\")\n"
      ],
      "metadata": {
        "id": "8Le01ZIAiYHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_evaluation(MODEL_DIR, TEST_CSV, SAVE_DIR, batch_size=32, max_length=256)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV2e7BI2ifOo",
        "outputId": "46005566-7bf6-4e45-cdf3-8b3fbd6ed8ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 313/313 [01:14<00:00,  4.18it/s]\n",
            "/tmp/ipython-input-939978410.py:91: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"timestamp\": datetime.datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] ✅ Evaluation complete. Reports saved to: /content/drive/MyDrive/quick_distilbert_model/eval_outputs\n"
          ]
        }
      ]
    }
  ]
}